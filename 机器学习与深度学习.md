**1、成本函数与损失函数区别**

- **损失函数(Loss function)**是定义在**单个训练样本上**的，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的，用L表示。当前也有应用到整个数据集上的损失。
- **代价函数(Cost function)**是定义在**整个训练集上面**的，也就是所有样本的误差的总和的平均，也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。只能应用到整个数据集，可以有更复杂的组合和计算，可以加上正则化项。

2、机器学习的目标是找到一个模型来近似真实映射函数g(x)或真实条件概率分布pr(y|x)。

**3、机器学习的三个基本要素**

- $\color{red}模型:$不知道真实的映射函数g(x)或条件概率分布pr(y|s)的具体形式，根据经验来确定一个假设函数集合F,称为**假设空间**，然后通过观测其在训练集D上的特性，从中选择一个理想的假设$$f^*\in F$$。常见的假设空间可以分为线性模型和非线性模型（如神经网络模型，**假设输入到输出的表达关系$f(x,\theta)$**）；

  假设空间F通常为一个参数化的函数族 $$F=\{f(x,\theta)|\theta\in R^m)\}$$ 。类似于神经网络训练过程中不同的参数集合。

- $\color{red}学习准则（损失函数）：$期望风险最小化，有0-1损失函数、交叉熵损失、平方差损失等。在训练数据集上，通过经验风险最小化来近似期望风险最小化，由于过拟合问题，通过增加正则化，设计结构风险最小化。**相当于设计目标函数（最优化问题）。** 学习准则有经验风险最小化、结构风险最小化、最大似然估计、最大后验估计等。

- $\color{red}优化算法：$在模型假设空间上找到最优化的模型，也就是找到使得结构风险最小**的参数集合**。如SGD算法。**（求解最优化问题）**  

**4、过拟合**

- 过拟合：根据大数定理，当训练集大小|D|趋向无穷大，经验风险就趋向于期望风险。然而通常，无法取得无限的训练    样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好地反映全部数据的真实分布。经验风险最小化原则很容易导致模型在训练数据集上错误率较低，而在测试集上错误率较高。

- **原因：**训练数据少、噪声、模型能力强等造成

**5、优化算法**

机器学习的训练过程其实就是最优化问题的求解过程

优化可以分为**参数优化**和**超参数优化**

- 参数优化：**模型$$f(x,\theta)$$** 中的$$\theta$$
- 超参数优化: 定义**模型结构或优化策略**的参数。常见的超参数包括如下
  - 聚类算法中的类别的个数
  - 梯度下降法的步长
  - 正则项的系数
  - 神经网络的层数
  - ...........

**6、梯度下降法** 

凸优化：选择合适模型和损失函数构造凸优化目标，从而利用一些高效、成熟的优化方法，如共轭梯度、拟牛顿法等

非凸优化：神经网络，只能找到局部最优解。

最简单、最常用的优化算法就是**梯度下降法**，通过迭代来计算训练集D上风险函数的最小值。

$$\theta_{t+1}=\theta_{t}-\alpha\frac{\alpha R_D(\theta)}{\alpha\theta}$$

$$=\theta_t-\alpha.\frac{1}{N}\sum^N_{n=1}\frac{\alpha L(y^{(n)},f(x^{n},\theta))}{\alpha \theta}$$



**7、提前终止**

**目的：** 防止过拟合

**过程：** 除了训练集和测试集之外，有时会采用一个验证集来进行模型选择，测试模型在验证集上是否最优。在每次迭代时，把最新得到的模型$$f(x,\theta)$$在验证集上进行测试，并计算错误率。如果在验证集上错误率不再下降，就停止迭代。这种策略叫做提前终止。

![提前终止示意图](F:\软件所工作\iscas-DL\机器学习与深度学习.assets/提前终止示意图.png)

**8、随机梯度下降法**

- **批量梯度下降法** （Batch Gradient Descent,BGD）:目标函数是整个训练集上风险函数。每次迭代需要计算每个样本上损失函数的梯度并求和。

  **缺陷：**当训练集样本数量N较大时，空间复杂度比较高，每次迭代计算开销很大。

- **随机梯度下降法（Stochastic Gradient Descent,SGD）:** 每次迭代只采集一个样本，计算这个样本损失函数的梯度并更新参数。经过足够次数的迭代时，随机梯度下降可以收敛到局部最优解。[Nemirovski et al., 2009]。一个缺点是无法充分利用计算机的并行计算能力。

![随机梯度下降算法](F:\软件所工作\iscas-DL\机器学习与深度学习.assets/随机梯度下降算法.png)

**两者区别**： 优化目标是所有样本的平均损失还是单个样本的损失。随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声。当目标函数非凸时，反而可以使其逃离局部最优点。

**小批量梯度下降法（Mini-Batch GD）:** 随机梯度和批量梯度的折中，每一次迭代选取一小部分训练样本来计算梯度并跟新参数。计算高效，收敛快，开销小，成为了大规模机器学习中的主要优化算法。

**8、线性回归**

假设空间是一组参数化的线性函数$$f(x;w,b)=w^Tx+b$$,其中w,b可以学习。

$$f(x;w,b)\in R$$称之为线性模型。

简单起见，可写成$$f(x,w)=\hat{w}^T.\hat{x}$$

其中，$$\hat{w}$$和$$\hat{x}$$ 分别称之为**增广权重向量**和**增广特征向量**

$$\hat{x}=x\bigoplus1\buildrel \Delta \over =\begin{bmatrix} \\\\x \\ \\\\ \\1\\ \end{bmatrix}=\begin{bmatrix} x_1\\.\\.\\.\\\\ x_k\\1\\ \end{bmatrix}$$



$$\hat{w}=w\bigoplus b\buildrel \Delta \over =\begin{bmatrix} \\\\w \\ \\\\ \\b\\ \end{bmatrix}=\begin{bmatrix} w_1\\.\\.\\.\\\\ w_k\\1\\ \end{bmatrix}$$

一般来说，后面直接用w和b代替增广权重向量和增广特征向量。

线性回归模型可以写成$$f(x;w)=w^Tx$$

- 参数学习

  给定一组包含N个训练样本的训练集$$D=\{({x^{(n)},y^{(n)}})\},1\le n \le N$$

  希望能够学习一个最优的线性回归的模型参数w。

  四种不同参数估计方法

  - 经验风险最小化
  - 结构风险最小化
  - 最大似然估计
  - 最大后验估计

- 经验风险最小化

​                   $$R(w)=\sum\limits_{n=1}^{N} {L(y^{(n)},f(x^{(n)},w))} \\$$

​                             $$=\frac{1}{2} \sum\limits_{n=1}^{N}{(y^{(n)}-w^Tx^{(n)})^2}$$	

​			     $$=\frac{1}{2}||y-X^Tw||^2$$		

其中$y\in R^N$是每个样本的真实标签$y^{(1)},\cdots,y^{(N)}$组成的列向量，$X\in R^{(d+1)\times N}$是所有输入$x^{(1)},\cdots,x^{(N)}$

组成的矩阵。**$$\color{red}矩阵的每一列代表一个样本的特征向量。$$**
$$
X=
\begin{bmatrix} 
	x_1^{(1)}&x_1^{(2)}& \cdots & x_1^{(N)}\\ 
	\vdots &\vdots & \ddots & \vdots\\ 
	x_d^{(1)}& x_d^{(2)}&\cdots& x_d^{(N)}\\
	1  & 1& \cdots &1  \ 
\end{bmatrix}
$$
风险函数R(w)是关于w的凸函数，其对w的偏导数为
$$
\begin{align}
\frac{\partial R(w)}{\partial w}&=\frac{1}{2}\frac{\partial||y-X^Tw||^2}{\part w} \\
 &  = -X(y-X^Tw)
 \end{align}
$$
令$\frac{\part}{\part w}R(w) =0$,得到最优的参数$w^*$为
$$
w^*=(XX^T)^{-1}Xy
$$
其中$(XX^T)^{-1}X$也称之为X的**伪逆矩阵**。



**9、线性代数基础** 

**向量：**一组实数组成的有序数组，同时具有大小和方向。

**向量空间（线性空间）：**指由向量组成的集合，满足以下两个条件：

- 向量加法：向量空间V中的两个向量a和b,它们的和a+b也属于空间V;
- 标量乘法：向量空间V中的任一向量a和任一标量c,它们的乘积$c\cdot a$也属于空间V

**范数：** 表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。对于一个n维向量v,一个常见的范数函数为$l_p​$范数，
$$
l_p\equiv||v||_p=(\sum\limits_{i=1}^{n}{|v_i|^p})^{1/p}
$$
其中$p\ge 0$为一个标量的参数。常用的p取值有1,2，$\infty $等。

- $l_1$范数：各个元素的绝对值之和
- $l_2$范数：根号下各个元素的平方和
- $l_\infty$范数：向量各个元素的最大绝对值

**10、矩阵**

**线性映射：** 指线性空间V到线性空间W的一个映射函数$f:V\to W$,并满足：对于V中任何两个向量u和v以及任何标量c,有
$$
f(u+v)=f(u)+f(v) \\

f(cv)=cf(v)
$$
两个有限维欧氏空间的映射函数$f:R^n\to R^m$可以表示为


$$
y=Ax \buildrel \Delta \over =
\begin{bmatrix}
	& a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_n \\
	& a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_n \\
	&\vdots \\
	& a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_n \\
\end{bmatrix}
$$

其中**A定义为$m\times n$的矩阵**，x为n维列向量，y为m维度列向量。

==**矩阵操作**==

- 加：A,B两个矩阵维度相同，都是$m\times n$,和的每个元素为A和B相应元素相加。

- 乘积: 假设两个A和B分别表示两个线性映射$g:R^m\to R^k$和$f:R^n\to R^m$,则其复合线性映射为
  $$
  (g\circ f)（x）=g(f(x))=g(Bx)=A(Bx)=(AB)x
  $$
  其中AB表示矩阵A和B的乘积，定义为
  $$
  [AB]_{ij}=\sum\limits_{k=1}^{m}{a_{ik}b_{kj}}
  $$
  **两个矩阵的乘积仅当第一个矩阵的列数与第二个矩阵的行数相等时才能定义。**

  A是$k\times m$矩阵，B是$m\times n$矩阵，AB是$k\times n$矩阵

  矩阵的乘法满足结合律和分配率

  - 结合律：（AB）C=A(BC)
  - 分配率：（A+B）C=AC+BC,C(A+B)=CA+CB

- Hadamard积 ：矩阵的逐点乘积，为A和B对应元素相乘。

- 转置：$m\times n$矩阵A的转置是一个$n\times m$的矩阵，记为$A^T$,$A^T$的第i行第j列的元素是原矩阵A的第j行第i列的元素。
  $$
  [A^T]_{ij}=[A]_{ji}
  $$

- **向量化：**矩阵的向量化是将矩阵表示为一个列向量。vec是向量化算子。设$A=[a_{ij}]_{m\times n}$,则
  $$
  vec（A）=[a_{11},a_{21},\cdots,a_{m1},a_{12},a_{22},\cdots,a_{m2},\cdots,a_{1n},a_{2n},\cdots,a_{mn}]T
  $$
  

- **迹**： **方块矩阵**A的对角线之和称之为它的迹，记为tr(A).尽管矩阵的乘法不满足交换律，但他们的迹相同，即tr(AB)=tr(BA)

- **行列式：** **方块矩阵**A的行列式是一个将其**映射到标量的函数**，记做det(A)或者|A|.行列式可看做有向面积或体积的概念在欧氏空间的推广。在n维欧氏空间中，行列式描述的是一个线性变换对“体积”所造成的影响。

- **秩**： 一个矩阵A的列秩是A线性无关的列向量数量，行秩是A的线性无关行向量数量。**一个矩阵的列秩和行秩总是相等的**，简称为**秩**。

  一个$m\times n$的矩阵的秩最大为min(m,n).两个矩阵的乘积AB的秩rank(AB)$\le $min(rank(A),rank(B)).

- **范数：**
  $$
  ||A||_p=(\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}{|a_{ij}|^p})^{1/p}
  $$


==**矩阵类型**==

- **对称矩阵：**转置之后等于自己的矩阵，即满足$A=A^T$

- **对角矩阵：**主对角线之外的元素皆为0的矩阵。对角线上的元素可以为0或者其他值。对角矩阵可以记为diag(a)，a为一个n维向量，并满足$[A]_{ii}=a_{i.}$.

  $n\times n$的对角矩阵A=diag(a)和n维度向量b的乘积为一个n维向量

  $$Ab=diag(a)b=a\odot b$,其中$\odot$表示点乘，即$(a\odot b)_i=a_ib_i$$

- **单位矩阵：**特殊的对角矩阵，其主对角元素为1，其余元素为0.n阶单元矩阵$I_n$,是一个$n\times n$的方块矩阵。可以记为$I_n=diag(1,1,\cdots,1)$

- **逆矩阵：** 对于一个$n\times n$的方框矩阵A，如果存在另一个方块矩阵B，使得

  AB=BA=$I_n$,则称A是可逆的。矩阵B称为A的逆矩阵，记为$A^{-1}$

  **一个方阵的行列式等于0当且仅当该方阵不可逆。**

- **正定矩阵：**对于一个**$n\times n $的对称矩阵**A,如果对于所有的非零向量$x\in R^n$都满足
  $$
  x^TAx>0
  $$
  则称A为正定矩阵。如果$x^TAx\ge0$则A是半正定矩阵。

  

- **正交矩阵**：A是一个方块矩阵，其逆矩阵等于其转置矩阵。
  $$
  A^T=A^{-1}
  $$
  即 $AA^T=A^TA=I_n$

  ==**特征值与特征向量**==

  如果一个标量$\lambda$和一个非零向量v 满足
  $$
  Av=\lambda v
  $$
  

  则$\lambda$和v分别称为矩阵A的特征值和特征向量。

  ==**矩阵分解**==

  **奇异值分解：**一个$m\times n$的矩阵A的奇异值分解（SVD）定义为
  $$
  A=U\sum V^T
  $$
  其中，U和V分别为$m\times m$和$n\times n$的**正交矩阵**,$\sum$为$m\times n$的对角矩阵，其对角线上的元素称之为**奇异值** 

  **特征分解：** 一个$n\times n$的方块矩阵A的特征分解定义为
  $$
  A=Q\wedge Q^{-1}
  $$
  其中Q为$n\times n$的**方块矩阵**，其每一列都为A的特征向量，为对角阵，其每一个对角元素为A的特征值。

  如果A为对称矩阵，则A可以分解为
  $$
  A=Q\wedge Q^T
  $$
  其中Q为**正交矩阵**

​     



















