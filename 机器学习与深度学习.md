## 一、机器学习概述

### 1、成本函数与损失函数区别

- **损失函数(Loss function)**是定义在**单个训练样本上**的，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的，用L表示。当前也有应用到整个数据集上的损失。
- **代价函数(Cost function)**是定义在**整个训练集上面**的，也就是所有样本的误差的总和的平均，也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。只能应用到整个数据集，可以有更复杂的组合和计算，可以加上正则化项。

### 2、机器学习目标

​       机器学习的目标是找到一个模型来近似真实映射函数g(x)或真实条件概率分布pr(y|x)。

### **3、机器学习的三个基本要素**

- $\color{red}模型:$不知道真实的映射函数g(x)或条件概率分布pr(y|s)的具体形式，根据经验来确定一个假设函数集合F,称为**假设空间**，然后通过观测其在训练集D上的特性，从中选择一个理想的假设$$f^*\in F$$。常见的假设空间可以分为线性模型和非线性模型（如神经网络模型，**假设输入到输出的表达关系$f(x,\theta)$**）；

  假设空间F通常为一个参数化的函数族 $$F=\{f(x,\theta)|\theta\in R^m)\}$$ 。类似于神经网络训练过程中不同的参数集合。

- $\color{red}学习准则（损失函数）：$期望风险最小化，有0-1损失函数、交叉熵损失、平方差损失等。在训练数据集上，通过经验风险最小化来近似期望风险最小化，由于过拟合问题，通过增加正则化，设计结构风险最小化。**相当于设计目标函数（最优化问题）。** 学习准则有经验风险最小化、结构风险最小化、最大似然估计、最大后验估计等。

- $\color{red}优化算法：$在模型假设空间上找到最优化的模型，也就是找到使得结构风险最小**的参数集合**。如SGD算法。**（求解最优化问题）**  

### **4、过拟合**

- 过拟合：根据大数定理，当训练集大小|D|趋向无穷大，经验风险就趋向于期望风险。然而通常，无法取得无限的训练    样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好地反映全部数据的真实分布。经验风险最小化原则很容易导致模型在训练数据集上错误率较低，而在测试集上错误率较高。

- **原因：**训练数据少、噪声、模型能力强等造成

### **5、优化算法**

机器学习的训练过程其实就是最优化问题的求解过程

优化可以分为**参数优化**和**超参数优化**

- 参数优化：**模型$$f(x,\theta)$$** 中的$$\theta$$
- 超参数优化: 定义**模型结构或优化策略**的参数。常见的超参数包括如下
  - 聚类算法中的类别的个数
  - 梯度下降法的步长
  - 正则项的系数
  - 神经网络的层数
  - ...........

### **6、梯度下降法** 

凸优化：选择合适模型和损失函数构造凸优化目标，从而利用一些高效、成熟的优化方法，如共轭梯度、拟牛顿法等

非凸优化：神经网络，只能找到局部最优解。

最简单、最常用的优化算法就是**梯度下降法**，通过迭代来计算训练集D上风险函数的最小值。

$$\theta_{t+1}=\theta_{t}-\alpha\frac{\alpha R_D(\theta)}{\alpha\theta}$$

$$=\theta_t-\alpha.\frac{1}{N}\sum^N_{n=1}\frac{\alpha L(y^{(n)},f(x^{n},\theta))}{\alpha \theta}$$



### **7、提前终止**

**目的：** 防止过拟合

**过程：** 除了训练集和测试集之外，有时会采用一个验证集来进行模型选择，测试模型在验证集上是否最优。在每次迭代时，把最新得到的模型$$f(x,\theta)$$在验证集上进行测试，并计算错误率。如果在验证集上错误率不再下降，就停止迭代。这种策略叫做提前终止。

![提前终止示意图](F:\软件所工作\iscas-DL\机器学习与深度学习.assets/提前终止示意图.png)

### **8、随机梯度下降法**

- **批量梯度下降法** （Batch Gradient Descent,BGD）:目标函数是整个训练集上风险函数。每次迭代需要计算每个样本上损失函数的梯度并求和。

  **缺陷：**当训练集样本数量N较大时，空间复杂度比较高，每次迭代计算开销很大。

- **随机梯度下降法（Stochastic Gradient Descent,SGD）:** 每次迭代只采集一个样本，计算这个样本损失函数的梯度并更新参数。经过足够次数的迭代时，随机梯度下降可以收敛到局部最优解。[Nemirovski et al., 2009]。一个缺点是无法充分利用计算机的并行计算能力。

![随机梯度下降算法](F:\软件所工作\iscas-DL\机器学习与深度学习.assets/随机梯度下降算法.png)

**两者区别**： 优化目标是所有样本的平均损失还是单个样本的损失。随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声。当目标函数非凸时，反而可以使其逃离局部最优点。

**小批量梯度下降法（Mini-Batch GD）:** 随机梯度和批量梯度的折中，每一次迭代选取一小部分训练样本来计算梯度并跟新参数。计算高效，收敛快，开销小，成为了大规模机器学习中的主要优化算法。

### **9、线性回归**

假设空间是一组参数化的线性函数$$f(x;w,b)=w^Tx+b$$,其中w,b可以学习。

$$f(x;w,b)\in R$$称之为线性模型。

简单起见，可写成$$f(x,w)=\hat{w}^T.\hat{x}$$

其中，$$\hat{w}$$和$$\hat{x}$$ 分别称之为**增广权重向量**和**增广特征向量**

$$\hat{x}=x\bigoplus1\buildrel \Delta \over =\begin{bmatrix} \\\\x \\ \\\\ \\1\\ \end{bmatrix}=\begin{bmatrix} x_1\\.\\.\\.\\\\ x_k\\1\\ \end{bmatrix}$$



$$\hat{w}=w\bigoplus b\buildrel \Delta \over =\begin{bmatrix} \\\\w \\ \\\\ \\b\\ \end{bmatrix}=\begin{bmatrix} w_1\\.\\.\\.\\\\ w_k\\1\\ \end{bmatrix}$$

一般来说，后面直接用w和b代替增广权重向量和增广特征向量。

线性回归模型可以写成$$f(x;w)=w^Tx$$

- 参数学习

  给定一组包含N个训练样本的训练集$$D=\{({x^{(n)},y^{(n)}})\},1\le n \le N$$

  希望能够学习一个最优的线性回归的模型参数w。

  四种不同参数估计方法

  - 经验风险最小化
  - 结构风险最小化
  - 最大似然估计
  - 最大后验估计

- 经验风险最小化

​                   $$R(w)=\sum\limits_{n=1}^{N} {L(y^{(n)},f(x^{(n)},w))} \\$$

​                             $$=\frac{1}{2} \sum\limits_{n=1}^{N}{(y^{(n)}-w^Tx^{(n)})^2}$$	

​			     $$=\frac{1}{2}||y-X^Tw||^2$$		

其中$y\in R^N$是每个样本的真实标签$y^{(1)},\cdots,y^{(N)}$组成的列向量，$X\in R^{(d+1)\times N}$是所有输入$x^{(1)},\cdots,x^{(N)}$

组成的矩阵。**$$\color{red}矩阵的每一列代表一个样本的特征向量。$$**
$$
X=
\begin{bmatrix} 
	x_1^{(1)}&x_1^{(2)}& \cdots & x_1^{(N)}\\ 
	\vdots &\vdots & \ddots & \vdots\\ 
	x_d^{(1)}& x_d^{(2)}&\cdots& x_d^{(N)}\\
	1  & 1& \cdots &1  \ 
\end{bmatrix}
$$
风险函数R(w)是关于w的凸函数，其对w的偏导数为
$$
\begin{align}
\frac{\partial R(w)}{\partial w}&=\frac{1}{2}\frac{\partial||y-X^Tw||^2}{\part w} \\
 &  = -X(y-X^Tw)
 \end{align}
$$
令$\frac{\part}{\part w}R(w) =0$,得到最优的参数$w^*$为
$$
\begin{align}
 w^* & =(XX^T)^{-1}Xy \\
& = (\sum\limits_{n=1}^{N}{x^{(n)}(x^{(n)})^T})^{-1}(\sum\limits_{n=1}^{N}{x^{(n)}y^{(n)}})
\end{align}
$$
其中$(XX^T)^{-1}X$也称之为X的**伪逆矩阵**。这种求解线性回归参数的方法也叫**最小二乘法估计（LSE）**

- 结构风险最小化

  - 经验风险最小化问题

    最小二乘法要求各个特征之间相互独立，保证$XX^T$可逆，但即使可逆条件下，如果特征之间可能会有较大**共线性**，也会使得$XX^T$的逆在数值上无法精确计算。数据集X的一些小的扰动会导致$(XX^T)^{-1}$发生大的改变，进而使得最小二乘法估计的计算变得很不稳定。

  - 解决方法：Hoerl提出了**领回归**，给$XX^T$的对角线元素都加上一个常数$\lambda I$使得（$XX^T+\lambda I$）的秩不为0.最优的参数$w^*$为
    $$
    w^*=(XX^T+\lambda I)^{-1}X\rm y
    $$


  领回归的解$w^*$可以看做**结构风险最小化准则下**的最小二乘法估计
$$
  R(w)=\frac{1}{2}||\rm y-X^Tw||^2+\frac{1}{2}||w||^2
$$
  其中$\lambda>0$为正则化系数。

- 

机器学习任务分两类：

1）样本的特征向量x和标签y之间的函数关系y=h(x)。

2）条件概率p(y|x)服从某个未知分布。

假设标签y为一个随机变量，其服从以均值为$f( \rm x,\rm w)=\rm w^T\rm x$为中心，方差为$\sigma^2$的高斯分布。
$$
\begin{align}
p(y|\rm x,\rm w,\sigma) &=N(y|\rm w^T\rm x,\sigma^2)\\
&=\frac{1}{\sqrt{2\pi}\sigma}\rm exp(-\frac{(y-w^T\rm x)^2}{2\sigma^2}).
\end{align}
$$
参数w在训练集D上的似然函数（likehodd）为
$$
\begin{align}
p(y|X,w,\sigma) &= \prod_{n=1}^{N} p(y^{(n)}|\rm x^{(n)},w,\sigma)\\
&=\prod_{n=1}^NN(y^{(n)}|\rm w^T\rm x^{(n)},\sigma^2)
\end{align}
$$
其中，${\rm {y}}=[y^{(1)},\dots,y^{(N)}]^T$为所有样本标签组成的向量，$X=[\rm x^{1},\dots,x^{(N)}]$为所有样本特征向量组成的矩阵

为了方便计算，对似然函数取对数，得到对数似然函数(log likelihood)
$$
{\rm log } p(\rm y|X,w,\sigma) = \sum\limits_{n=1}^N {log N(y^{(n)}|\rm w^T\rm x^{(n)},\sigma^2)}
$$
**最大似然估计（MLE）**是指找到一组参数w使得似然函数$p(\rm y|X,w,\sigma)$最大，等价于对数似然函数最大。

令$$\frac{\part {\rm log}p(\rm y|X,w,\sigma)}{\part w}=0$$,得到
$$
w^{ML}=(XX^T)^{-1}X\rm y
$$
最大似然估计的解和最小二乘估计的解相同。

- 最大后验估计

  假设参数w为一个随机向量，并服从一个先验分布$p(w|v)$.一般令$p(w|v)$为各向同性的高斯分布
  $$
  p(w|v)=N(w|0,v^2I)
  $$
  其中$v^2$为每一维上的方差。

  根据贝叶斯公式，参数w的后验概率分布为
  $$
  \begin{align}
  p\rm(w|X,y,\nu,\sigma)&=\frac{p(w,y|X,\nu,\sigma)}{\sum_wp(w,y|X,\nu,\sigma)}\\
  &\propto p(\rm y|X,w,\sigma)p(w|\nu)
  \end{align}
  $$
  其中$p\rm(y|X,w,\sigma)$称之为似然函数，$p(\rm w|\nu)$为先验概率。

### **10、线性代数基础** 

**向量：**一组实数组成的有序数组，同时具有大小和方向。

**向量空间（线性空间）：**指由向量组成的集合，满足以下两个条件：

- 向量加法：向量空间V中的两个向量a和b,它们的和a+b也属于空间V;
- 标量乘法：向量空间V中的任一向量a和任一标量c,它们的乘积$c\cdot a$也属于空间V

**范数：** 表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。对于一个n维向量v,一个常见的范数函数为$l_p$范数，
$$
l_p\equiv||v||_p=(\sum\limits_{i=1}^{n}{|v_i|^p})^{1/p}
$$
其中$p\ge 0$为一个标量的参数。常用的p取值有1,2，$\infty $等。

- $l_1$范数：各个元素的绝对值之和
- $l_2$范数：根号下各个元素的平方和
- $l_\infty$范数：向量各个元素的最大绝对值

---------

**10、矩阵**

**线性映射：** 指线性空间V到线性空间W的一个映射函数$f:V\to W$,并满足：对于V中任何两个向量u和v以及任何标量c,有
$$
f(u+v)=f(u)+f(v) \\

f(cv)=cf(v)
$$
两个有限维欧氏空间的映射函数$f:R^n\to R^m$可以表示为


$$
y=Ax \buildrel \Delta \over =
\begin{bmatrix}
	& a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_n \\
	& a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_n \\
	&\vdots \\
	& a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_n \\
\end{bmatrix}
$$

其中==**A定义为$m\times n$的矩阵**==，x为n维列向量，y为m维度列向量。

==**矩阵操作**==

- 加：A,B两个矩阵维度相同，都是$m\times n$,和的每个元素为A和B相应元素相加。

- 乘积: 假设两个A和B分别表示两个线性映射$g:R^m\to R^k$和$f:R^n\to R^m$,则其复合线性映射为
  $$
  (g\circ f)（x）=g(f(x))=g(Bx)=A(Bx)=(AB)x
  $$
  其中AB表示矩阵A和B的乘积，定义为
  $$
  [AB]_{ij}=\sum\limits_{k=1}^{m}{a_{ik}b_{kj}}
  $$
  **两个矩阵的乘积仅当第一个矩阵的列数与第二个矩阵的行数相等时才能定义。**

  A是$k\times m$矩阵，B是$m\times n$矩阵，AB是$k\times n$矩阵

  矩阵的乘法满足结合律和分配率

  - 结合律：（AB）C=A(BC)
  - 分配率：（A+B）C=AC+BC,C(A+B)=CA+CB

- Hadamard积 ：矩阵的逐点乘积，为A和B对应元素相乘。

- 转置：$m\times n$矩阵A的转置是一个$n\times m$的矩阵，记为$A^T$,$A^T$的第i行第j列的元素是原矩阵A的第j行第i列的元素。
  $$
  [A^T]_{ij}=[A]_{ji}
  $$

- **向量化：**矩阵的向量化是将矩阵表示为一个列向量。vec是向量化算子。设$A=[a_{ij}]_{m\times n}$,则
  $$
  vec（A）=[a_{11},a_{21},\cdots,a_{m1},a_{12},a_{22},\cdots,a_{m2},\cdots,a_{1n},a_{2n},\cdots,a_{mn}]T
  $$

- **迹**： **方块矩阵**A的对角线之和称之为它的迹，记为tr(A).尽管矩阵的乘法不满足交换律，但他们的迹相同，即tr(AB)=tr(BA)

- **行列式：** **方块矩阵**A的行列式是一个将其**映射到标量的函数**，记做det(A)或者|A|.行列式可看做有向面积或体积的概念在欧氏空间的推广。在n维欧氏空间中，行列式描述的是一个线性变换对“体积”所造成的影响。

- **秩**： 一个矩阵A的列秩是A线性无关的列向量数量，行秩是A的线性无关行向量数量。**一个矩阵的列秩和行秩总是相等的**，简称为**秩**。

  一个$m\times n$的矩阵的秩最大为min(m,n).两个矩阵的乘积AB的秩rank(AB)$\le $min(rank(A),rank(B)).

- **范数：**
  $$
  ||A||_p=(\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}{|a_{ij}|^p})^{1/p}
  $$



==**矩阵类型**==

- **对称矩阵：**转置之后等于自己的矩阵，即满足$A=A^T$

- **对角矩阵：**主对角线之外的元素皆为0的矩阵。对角线上的元素可以为0或者其他值。对角矩阵可以记为diag(a)，a为一个n维向量，并满足$[A]_{ii}=a_{i.}$.

  $n\times n$的对角矩阵A=diag(a)和n维度向量b的乘积为一个n维向量

  $$Ab=diag(a)b=a\odot b$,其中$\odot$表示点乘，即$(a\odot b)_i=a_ib_i$$

- **单位矩阵：**特殊的对角矩阵，其主对角元素为1，其余元素为0.n阶单元矩阵$I_n$,是一个$n\times n$的方块矩阵。可以记为$I_n=diag(1,1,\cdots,1)$

- **逆矩阵：** 对于一个$n\times n$的方框矩阵A，如果存在另一个方块矩阵B，使得

  AB=BA=$I_n$,则称A是可逆的。矩阵B称为A的逆矩阵，记为$A^{-1}$

  **一个方阵的行列式等于0当且仅当该方阵不可逆。**

- **正定矩阵：**对于一个**$n\times n $的对称矩阵**A,如果对于所有的非零向量$x\in R^n$都满足
  $$
  x^TAx>0
  $$
  则称A为正定矩阵。如果$x^TAx\ge0$则A是半正定矩阵。

  

- **正交矩阵**：A是一个方块矩阵，其逆矩阵等于其转置矩阵。
  $$
  A^T=A^{-1}
  $$
  即 $AA^T=A^TA=I_n$

  

  ==**特征值与特征向量**==

  如果一个标量$\lambda$和一个非零向量v 满足
  $$
  Av=\lambda v
  $$


  则$\lambda$和v分别称为矩阵A的特征值和特征向量。

  ==**矩阵分解**==

  **奇异值分解：**一个$m\times n$的矩阵A的奇异值分解（SVD）定义为
$$
  A=U\sum V^T
$$
  其中，U和V分别为$m\times m$和$n\times n$的**正交矩阵**,$\sum$为$m\times n$的对角矩阵，其对角线上的元素称之为**奇异值** 

  **特征分解：** 一个$n\times n$的方块矩阵A的特征分解定义为
$$
  A=Q\wedge Q^{-1}
$$
  其中Q为$n\times n$的**方块矩阵**，其每一列都为A的特征向量，为对角阵，其每一个对角元素为A的特征值。

  如果A为对称矩阵，则A可以分解为
$$
  A=Q\wedge Q^T
$$
  其中Q为**正交矩阵**

----

### **11、微积分**

**偏导数：**对于一个多变量函数$f:R^d\to R$，它的偏导数是关于其中一个变量$x_i$的导数，而其他变量保持固定，可以记为$f_{x_i}^{'}(x),\nabla_{x_i}f(x),\frac{\part f(x)}{\part x_i}$

对于一个d维向量$\rm{x}\in R^d$,函数$f(\rm{x})=f(x_1,\cdots,x_d)\in R$,则f(x)关于x的偏导数为
$$
\frac{\part f(\rm{x})}{\part \rm{x}}=
\begin{bmatrix}
	\frac{\part f(x)}{\part x_1} \\
	\vdots \\
	\frac{\part f(x)}{\part x_d} \\
\end{bmatrix} \in R^d.
$$
若函数$f(\rm{x})\in R^k$的值也是一个向量，则f(x)关于x的偏导数为
$$
\frac{\part f(\rm{x})}{\part {\rm x}}=
\begin {bmatrix}
	\frac{\part f_1(\rm x)}{\part x_1} &  \cdots  & \frac{\part f_k(\rm x)}{\part x_1}  \\
	\vdots  & \vdots & \vdots \\
	\frac{\part f_1(\rm x)}{\part x_d} &  \cdots  & \frac{\part f_k(\rm x)}{\part x_d}  \\
\end {bmatrix}
\in R^{d\times k.}
$$
​    称之为**==Jacobian矩阵==。**

**导数法则**

- 加减法则

  $\rm{y}=f(\rm{x}),z=\rm{g}(\rm{x})$,则
  $$
  \frac{\part{(\rm{y}+z)}}{\part \rm{x}}=\frac{\part \rm{y}}{\part{\rm x}}+\frac{\part \rm{z}}{\part{\rm x}}
  $$

- 乘法法则

  (1)若$\rm{x}\in R^p,\rm{y}=f(\rm x)\in R^q，\rm z=\rm g(\rm x)\in R^q$则
  $$
  \frac{\part\rm y^T\rm z}{\part \rm x}=\frac{\part \rm y}{\part \rm x}\rm z+\frac{\part z}{\part \rm x}\rm y.
  $$

  > 分子为实数值

  (2)若$\rm{x}\in R^p,\rm{y}=f(\rm x)\in R^s，\rm z=\rm g(\rm x)\in R^t,A\in R^{s\times t}$和x无关，则
  $$
  \frac{\part\rm y^T A\rm z}{\part \rm x}=\frac{\part \rm y}{\part \rm x}A\rm z+\frac{\part z}{\part \rm x}A^T\rm y.
  $$

  > 分子为实数值

  (3)若$\rm{x}\in R^p,y=f(\rm x)\in R，\rm z=\rm g(\rm x)\in R^p,$则
  $$
  \frac{\part\rm y\rm z}{\part \rm x}=\frac{\part \rm z}{\part \rm x}y+\frac{\part y}{\part \rm x}\rm z^T.
  $$

- 向量函数及导数
  $$
  \frac{\part \rm x}{\part\rm x}=I,\\
  \frac{\part A\rm x}{\part \rm x}=A^T,\\
  \frac{\part\rm x^TA}{\part \rm x}=A
  $$
  

### 12、偏差与方差

如何在模型能力和复杂度之间取得一个较好的平衡对一个机器学习算法来讲十分重要。**偏差-方差**分解（Bias-Variance Decomposition）为我们提供一个很好的分析和指导工具。

**偏差（bias）**：是指一个模型的在不同训练集上的平均性能和最优模型的差异。偏差可以用来衡量一个模型的拟合能力；

**方差（variance）**：是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。



- 随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而导致过拟合。

- 低偏差高方差的情况，表示模型的拟合能力很好，但是泛化能力比较差。

- 方差一般会随着训练样本的增加而减少。**当样本比较多时，方差比较少**，我们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往往都比较有限，最优的偏差和最优的方差就无法兼顾。

![偏差_方差与模型复杂度关系曲线](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\偏差_方差与模型复杂度关系曲线.png)

- **过拟合原因及解决方法**：当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型复杂度，加大正则化系数，引入先验等方法来缓解。

- **欠拟合原因及解决方法**：当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型、



### 13、机器学习算法类型

按模型分（f(x)划分）：线性模型和非线性模型；

学习准则（损失函数）：统计学习方法和非统计学习方法；

一般来说，从训练样本的信息及反馈的方式，可以分为**监督学习、非监督学习、强化学习**

**监督学习：**如果机器学习的目标是通过建模样本的特征x 和标签y 之间的关系：y = f(x, θ) 或p(y|x, θ)，并且训练集中每个样本都有标签，那么这类机器学习称为监督学习（Supervised Learning）。根据标签类型的不同，可以分为分类和回归。

> 分类标签离散，回归的标签连续；



**非监督学习：**无监督学习（Unsupervised Learning，UL）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有聚类、密度估计、特征学习、降维等。

**强化学习：**强化学习（Reinforcement Learning，RL）是一类通过交互来学习的机器学习算法。在强化学习中，智能体根据环境的状态做出一个动作，并得到即时或延时的奖励。智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报。

![3种机器学习类型比较](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\3种机器学习类型比较.png)

### 14、数据的特征学习

特征空间：一张灰度图像，有n个像素，其特征空间大小为$$[0,255]^n$$ 

原始特征的不足：

（1）特征比较单一，需要进行（非线性的）组合才能发挥其作用；

（2）特征之间冗余度比较高；

（3）并不是所有的特征都对预测有用；

（4）很多特征通常是易变的；

（5）特征中往往存在一些噪声。

**特征工程**：为了提高机器学习算法的能力，我们需要抽取有效、稳定的特征。传统的特征提取是通过人工方式进行的，需要大量的人工和专家知识。一个成功的机器学习系统通常需要尝试大量的特征，称为特征工程（Feature Engineering）

**表示学习**：如何让机器自动地学习出有效的特征也成为机器学习中的一项重要研究内容，称为**特征学习**，也叫**表示学习**。表示学习可以看作是一个特殊的机器学习任务，即有自己的模型、学习准则和优化方法。特征学习在一定程度上也可以减少预测模型复杂性、缩短训练时间、提高模型泛化能力、避免过拟合等。

传统的特征学习一般是通过人为地设计一些准则，然后根据这些准则来选取有效的特征，具体又可以分为两种：**特征选择和特征抽取。**

**特征选择：**（Feature Selection）是选取原始特征集合的一个有效子集，使得基于这个特征子集训练出来的模型准确率最高。

**特征抽取：**构造一个新的特征空间，并将原始特征投影到新的空间中去。可以分为监督和无监督方法，传统的特征选择和提取方法如下表。

![传统特征选择与特征提取方法](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\传统特征选择与特征提取方法.png)

**特征选择和特征抽取的优点**：是可以用较少的特征来表示原始特征中的大部分相关信息，去掉噪声信息，并进而提高计算效率和减小维度灾难（Curse OfDimensionality）

### 15、机器学习评价指标

>  为了衡量机器学习模型的好坏，需要给定一个测试集，用模型对测试集中的每一个样本进行预测，并根据预测结果计算评价分数。

- 分类问题

  正确率、准确率、召回率和F值等

  给定测试集合$$ T=(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)},...,(x^{(N)},y^{(N)}))$$,假设标签$$y^{(n)}\in{1,...,C}$$

  用学习好的模型$$f(x, \theta)$$对测试集中的每一个样本进行预测，结果为$$Y=\hat y^{(1)},...,\hat y^{(N)}$$

  **1）准确率**

  $$ACC=\frac {1}{N}\sum_{n=1}^{N} I(y^{(n)}=\hat y^{(n)})$$

  I(.)为指示函数

  **2）错误率**

  error = 1 -ACC

- **查准率和查全率：对每个类都进行性能估计**

  准确率是所有类别整体性能的平均

  对于类别c来说，模型在测试集上的结果可以分为以下四种情况：

  1、**真实例（True Positive,TP）**:一个样本的真实类别为c,并且模型正确预测为类别c。这样样本数量记为

  $$TP_c=\sum_{n=1}^{N} I(y^{(n)}=\hat y^{(n)}=c)$$.

  2、**假负例（False Negative,FN）**:一个样本的真实类别为c,模型错误地预测为其他类。这类样本数量记为

  $$FN_c=\sum_{n=1}^{N} I(y^{(n)}=c  \bigwedge \hat y^{(n)}\ne c)$$.

  3、**假正例（False Positive,FP）:** 一个样本的真实类别为其他类，模型错误地预测为类c。这样样本数量记为

  $$FP_c=\sum_{n=1}^{N} I(y^{(n)}\ne c \bigwedge  \hat y^{(n)}=c)$$

  4、**真负例（True Negative,TN）**:一个样本的真实类别为其他类，模型也预测为其他类。这类样本数量记为$TN_c$。对于类别c来说，这种情况一般不需要关注。

  ![类别c的预测结果的混淆矩阵](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\类别c的预测结果的混淆矩阵.png)

  $$\color \red查准率（Precision）:$$ 也叫精确率或者精度，类别c的查准率为是所有预测为类别c的样本，预测正确的比例。
  $$
  P_c = \frac{TP_c}{TP_c+FP_c}
  $$
  **$$\color \red{查全率}(Recall):$$** 也叫召回率，类别c的查全率为所有真实标签为类别c的样本中，预测正确的比例。
  $$
  R_c = \frac{TP_c}{TP_c+FN_c}
  $$
   $$\color \red{F值}$$：综合指标，为查准率和查全率的调和平均
  $$
  F_c =\frac{(1+\beta^2)\times P_c\times R_c}{\beta^2\times P_c+R_c}
  $$
  其中$$\beta$$用于平衡查全率和查准率的重要性，一般取值为1。$$\beta=1$$时候的F值称为$$F_1$$值，是查准率和查全率的调和平均。

  

  **交叉验证：**交叉验证（Cross Validation）是一种比较好的可能衡量机器学习模型的统计分析方法，可以有效避免划分训练集和测试集时的随机性对评价结果造成的影响。我们可以把原始数据集平均分为K 组不重复的子集，每次选K − 1组子集作为训练集，剩下的一组子集作为验证集。**这样可以进行K 次试验并得到K个模型。这K个模型在各自验证集上的错误率的平均作为分类器的评价。**

  -----------------

  交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　

    

  那么什么时候才需要交叉验证呢？交叉验证用在数据不是很充足的时候。比如在我日常项目里面，对于普通适中问题，如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型。如果样本大于一万条的话，我们一般随机的把数据分成三份，一份为训练集（Training Set），一份为验证集（Validation Set），最后一份为测试集（Test Set）。用训练集来训练模型，用验证集来评估模型预测的好坏和选择模型及其对应的参数。把最终得到的模型再用于测试集，最终决定使用哪个模型以及对应参数。

   回到交叉验证，根据切分的方法不同，交叉验证分为下面三种：　　　

   

  第一种是**简单交叉验证**，所谓的简单，是和其他交叉验证方法相对而言的。首先，我们随机的将样本数据分为两部分（比如： 70%的训练集，30%的测试集），然后用训练集来训练模型，在测试集上验证模型及参数。接着，我们再把样本打乱，重新选择训练集和测试集，继续训练数据和检验模型。最后我们选择损失函数评估最优的模型和参数。　

   

   第二种是**S折交叉验证**（S-Folder Cross Validation）。和第一种方法不同，S折交叉验证会把样本数据随机的分成S份，每次随机的选择S-1份作为训练集，剩下的1份做测试集。当这一轮完成后，重新随机选择S-1份来训练数据。若干轮（小于S）之后，选择损失函数评估最优的模型和参数。（）

   

  第三种是**留一交叉验证**（Leave-one-out Cross Validation），它是第二种情况的特例，此时S等于样本数N，这样对于N个样本，每次选择N-1个样本来训练数据，留一个样本来验证模型预测的好坏。此方法主要用于样本量非常少的情况，比如对于普通适中问题，N小于50时，我一般采用留一交叉验证。

   

  通过反复的交叉验证，用损失函数来度量得到的模型的好坏，最终我们可以得到一个较好的模型。那这三种情况，到底我们应该选择哪一种方法呢？一句话总结，如果我们只是对数据做一个初步的模型建立，不是要做深入分析的话，简单交叉验证就可以了。否则就用S折交叉验证。在样本量少的时候，使用S折交叉验证的特例留一交叉验证。

   

  此外还有一种比较特殊的交叉验证方式，也是用于样本量少的时候。叫做**自助法**(bootstrapping)。比如我们有m个样本（m较小），每次在这m个样本中随机采集一个样本，放入训练集，采样完后把样本放回。这样重复采集m次，我们得到m个样本组成的训练集。当然，这m个样本中很有可能有重复的样本数据。同时，用没有被采样到的样本做测试集。这样接着进行交叉验证。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。

  -------

  

  **机器学习学派**

  目前机器学习中最主流的一类方法是统计学习方法，将机器学习问题看作是统计推断问题，并且又可以进一步分为频率学派和贝叶斯学派。频率学派将模型参数θ 看作是固定常数；而贝叶斯学派将参数θ 看作是随机变量，并且存在某种先验分布。

  

  **问题：**

  **1、分类问题中为什么不用平方损失？**

  答：标签为整数值，预测输出也为整数值。这个值只代表属于某一类，并不代表一定的数学意义，比如真是标签y=2,表示非洲象,y=100表示印度大象。如果一张非洲象的图片被预测为印度大象，两者其实在物理意义上比较相似，但是经过数学均方差计算后，差值较大。所以采用均方差表示分类的损失时候，物理意义和数学意义不太对等。

  **2、线性回归前面加一个权重的目的？**

  模型预测的输出和实际的标签是存在一定的误差，有的样本点与模型的误差较大，有的样本点与模型的误差较小。加上一个权重，可以让我们更加关注误差较小的样本点，让模型逼近这些点。而误差较大，离直线较远的点，选择忽略。对于误差较小的样本点，选择较大的权重。

  ------

  

## 二、线性模型

四种线性分类模型：主要区别在于使用了**不同的损失函数**

1）logistic回归

2）softmax回归

3）感知器

4）支持向量机

### 16、线性判别函数和决策边界

$$
y=g(f(x,w))
$$

- 线性判别函数：$$f(x,w)=w^Tx+b$$

- 非线性决策函数g(.)：比如符号函数。

线性模型是机器学习中应用最广泛的模型，指通过样本特征的线性组合来进行预测的模型。

决策边界：特征空间$R^d$中所有满足f(x,w)=0的点组成用一个分割超平面，称为**决策边界或决策平面**.所谓“线性分类模型”就是指其决策边界是线性超平面。

![1562075176870](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562075176870.png)

多分类线性可分

![1562076967082](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562076967082.png)

![1562636498188](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562636498188.png)

### 17、logistic 回归

- 线性模型
- 二分类问题
- $y\in{0,1}$
- 损失函数：交叉熵损失
- 优化算法：梯度下降

线性函数不适合做分类，引入非线性函数(激活函数)$g:R^d->(0,1)$来预测类别标签的后验概率p(y=1|x)

逻辑回归中，采用logistic函数来作为激活函数。激活函数将线性函数的值域从实数区间“挤压”到（0,1）之间，可以用来表示概率
$$
\begin{align}
p(y=1|x)=\delta(w^T\rm x) \
&  =  \frac{1}{1+exp(-w^T\rm x)}
\end{align}
$$
logistic函数及导数如下：
$$
\delta(x)=\frac{1}{1+exp(\rm -x)}
$$

$$
\delta^{'}(x)=\delta(x)(1-\delta(x))
$$

**参数学习**

![1562245865220](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562245865220.png)

![1562247109015](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562247109015.png)

![1562586415007](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562586415007.png)

### 18、softmax回归

softmax回归也称为多项或多累的logistic回归，是logistic回归在多分类问题上的推广。

对于多分类问题，类别标签$y \in{1,2,...,C}$可以有C个取值。给定一个样本x,softmax回归预测的属于类别c的条件概率为：
$$
\begin{align}
p(y=c|\rm x) &= softmax(w_c^T\rm x) \\
 &= \frac{exp(w_c^T\rm x)}{\sum_{c=1}^{C}exp(w_c^T\rm x)}
\end{align}
$$
其中$w_c$是第c类的权重向量。

softmax回归的决策函数可以表示为：
$$
\begin{align}
\hat y &=\mathop {\arg \max }\limits_{c = 1}^C  p(y=c|x)\\
&=\mathop {\arg \max }\limits_{c = 1}^C w_c^T\rm x\
\end{align}
$$
![1562594262485](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562594262485.png)

![1562593979773](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562593979773.png)

![1562594723093](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562594723093.png)



![1562594753585](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562594753585.png)

### 19、感知器

- 最简单的人工神经网络，只有一个神经元。输出+1或者-1.

- 简单的二类线性分类模型，分类准则与公式如下:

$$
\hat y=\rm sgn(\rm w^T\rm x)
$$

给定N个样本的训练集：${x^{(n)},y^{(n)}}_{n=1}^{N},$其中$y^{(n)}\in \{+1,-1\}$,感知器试图学习到参数$w^*$ ,使得对于每个样本$(x^{(n)},y^{(n)})$有
$$
y^{(n)}\rm w^*x^{(n)}>0, \forall n\in[1,N].
$$
错误驱动的在线学习算法。每次分错一个样本(x,y)时，即$y\rm w^Tx<0$，就用这个样本来更新权重。
$$
\rm {w\leftarrow w}+yx
$$
![1562635675140](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562635675140.png)

感知器损失函数为：
$$
L(w;x,y)=max(0,-y\rm w^Tx)
$$
采用随机梯度下降，其每次更新的梯度为

![1562636252659](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1562636252659.png)



虽然感知器在线性可分的数据上可以保证收敛，但其存在以下不足之处：

- 在数据集线性可分时，感知器虽然可以找到一个超平面把两类数据分开，
  但并不能保证能其泛化能力。

- 感知器对样本顺序比较敏感。每次迭代的顺序不一致时，找到的分割超平
  面也往往不一致。

- 如果训练集不是线性可分的，就永远不会收敛[Freund and Schapire, 1999]。

### 20、支持向量机（SVM）

给定一个两类分类器数据集$D={（\rm x^{(n)}，y^{(n)}})_{n=1}^N$,其中$y_n \in \{ +1,-1\}$

如果两类样本是线性可分的，即存在一个超平面
$$
\rm w^Tx+b=0
$$
将二类样本分开，那么对于每个样本都有$y^{(n)}(\rm w^Tx^{(n)}+b)>0)$

数据集D中每个样本 $\rm x^{(n)}$到超平面的距离为：
$$
r^{(n)}=\frac{||\rm w^Tx^{(n)}+b||}{||\rm w||}= \frac{y^{(n)}(\rm w^Tx^{(n)}+b)}{||\rm w||}.
$$
定义真个数据集D中所有样本到分割超平面的最短距离为**间隔(Margin)**$\gamma$
$$
\gamma=\mathop {\min }\limits_{n}\gamma^{(n)}
$$
间隔越大，其分割超平面对两个数据集的划分越稳定，不容易受到噪声等因素影响。**支持向量机的目标是寻找一个超平面$(w^*,b^*)$使得$\gamma$ 最大**，即
$$
\begin{align}
&\mathop{\max}_{w,b}{\gamma}\\
& s.t.\frac{y^{(n)}(\rm w^Tx^{(n)}+b)}{||\rm w||}\ge\gamma,\forall n
\end{align}
$$

**令$||w||.\gamma =1$**，则等价于
$$
\begin{align}
&\mathop{\max}_{w,b}{\frac{1}{||w||^2}}\\
& s.t.{y^{(n)}(\rm w^Tx^{(n)}+b)}\ge 1,\forall n
\end{align}
$$
数据集中所有满足$y^{(n)}(\rm w^Tx^{(n)}+b)=1$的样本点，都称为**支持向量（Support Vector）**

![1564987473171](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564987473171.png)**a.参数学习**

为了找到最大间隔分割超平面，将公式的目标函数写成凸优化问题
$$
\begin{align}
&\mathop{\min}_{w,b}{\frac{1}{2}||w||^2}\\
& s.t.  1-{y^{(n)}(\rm w^Tx^{(n)}+b)}\le0, \forall n
\end{align}
$$
使用拉格朗日乘数法，**拉格朗日函数**为

$% MathType!MTEF!2!1!+-
% feaagKart1ev2aqatCvAUfeBSjuyZL2yd9gzLbvyNv2CaerbuLwBLn
% hiov2DGi1BTfMBaeXatLxBI9gBaerbd9wDYLwzYbItLDharqqtubsr
% 4rNCHbWexLMBbXgBd9gzLbvyNv2CaeHbl7mZLdGeaGqiVu0Je9sqqr
% pepC0xbbL8F4rqqrFfpeea0xe9Lq-Jc9vqaqpepm0xbba9pwe9Q8fs
% 0-yqaqpepae9pg0FirpepeKkFr0xfr-xfr-xb9adbaqaaeGaciGaai
% aabeqaamaabaabauaakeaacqGHNis2aaa!40D0!
$ $\wedge (\rm w,b,\lambda) =\frac{1}{2}||w||^2+\sum\limits_{n=1}^N\lambda_n(1-y^{(n)}(w^{T}x^{(n)}+b)),$

其中$\lambda_1\ge0,...,\lambda_N\ge0$ 为拉格朗日乘数。计算$\wedge (\rm w,b,\lambda) $关于w,b的导数，并令其等于0得到
$$
\begin{align}
& \rm w=\sum\limits_{n=1}^N\lambda_ny^{(n)}x^{(n)},\\
& 0=\sum\limits_{n=1}^N\lambda_ny^{(n)}.
\end{align}
$$


![1564993354996](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564993354996.png)

![1564994394296](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564994394296.png)

![1564997898373](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564997898373.png)

其他网友推导如下：

![1564993379047](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564993379047.png)





![1564993395864](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564993395864.png)



**优化方法**

对偶函数可以形式为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解，并且许多最优化算法可以用于这一问题的求解。但是当训练样本容量很大时，这些算法往往变得非常低效，以致无法使用。序列最小优化算法（SMO）

![1564994507005](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564994507005.png)

![1564994553440](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564994553440.png)

**SMO算法如下：**

![1564994732333](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1564994732333.png)

先计算出$\lambda $ ,再计算w,b

- **核函数**

隐式地将样本从原始特征空间映射到更高维的空间，并解决原始特征空间中的线性不可分问题。

通过一个非线性变换，将非线性问题转换成线性问题。

![1565054190932](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565054190932.png)



- 软间隔

![1565054226261](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565054226261.png)

参考：https://zhuanlan.zhihu.com/p/35755150

https://blog.csdn.net/chaipp0607/article/details/73849539

李航--《统计学习方法》



### 21、模型对比与总结

**损失函数对比**：logistic回归、感知器、支持向量机

三者使用了相同的决策函数，但由于使用了不同的损失函数以及对应的优化方法，导致它们之间在实际任务上的表现存在一定的差异。

![1565054851196](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565054851196.png)

![1565054873423](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565054873423.png)

## 十四、强化学习

![1565056173469](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565056173469.png)

强化学习（也叫增强学习），是指一类与环境交互中不断学习的问题以及解决这类问题的方法。可以描述为一个智能体从与环境的交互中不断学习以完成特定目标（比如取得最大化奖励值）。

和深度学习类似，强化学习中的关键问题也是【**贡献度分配问题**，1963]，每一个动作并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性。

> 贡献度分配问题即一个系统
> 中不同的组件（components）
> 对最终系统输出结果的贡献
> 或影响。

### **21、强化学习定义**

- **智能体（agent）**可以感知外界环境的状态（state）和反馈的奖励（reward），
  并进行学习和决策。
  智能体的**决策**功能是指根据外界环境的状态来做出不同的动作（action），
  而**学习**功能是指根据外界环境的奖励来调整策略。
- **环境（environment）**是智能体外部的所有事物，并受智能体动作的影响
  而改变其状态，并反馈给智能体相应的奖励。

![1565057410821](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565057410821.png)

**策略**：智能体的策略就是智能体如何根据环境状态s来决定下一步的动作a,通常可以分为**确定性策略（DP）**和**随机性策略（SP）**两组



![1565057561264](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565057561264.png)

**2）马尔科夫决策过程**

简单起见，将智能体和环境的交互看作是离散的时间序列。如下图所示。

![1565057849041](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565057849041.png)



![1565058080584](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565058080584.png)

智能体与环境的交互的过程可以看做是一个马尔科夫决策过程。

![1565058157883](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565058157883.png)

![1565058380797](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565058380797.png)

![1565058473040](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565058473040.png)

**3）目标函数**

![1565058945811](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565058945811.png)



![1565059353021](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565059353021.png)



![1565062204133](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565062204133.png)

![1565062222479](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565062222479.png)

![1565062235241](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565062235241.png)

![1565062462842](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565062462842.png)

**14.1.6 深度强化学习**

-  强化学习来定义问题和优化目标

- 深度学习来解决策略额值函数的建模问题，然后使用误差反向传播算法来优化目标函数。

### **22、基于值函数的学习方法**

值函数是对策略$\pi$的评估，如果策略$\pi$ 有限（即状态数和动作数都有限）时，可以对所有策略进行评估并选出最优策略$\pi^*$.

![1565071843090](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565071843090.png)

**基于值函数的策略学习方法中最关键的是如何计算策略$\pi$的值函数，**一般采用两种方法：

- **动态规划**
- **蒙特卡罗**

### **23、**动态规划算法（**基于模型**）

基于模型的强化学习：模型指的是**马尔科夫决策过程**，具体指状态转移概率$p(s^{'}|s,a)$

和奖励$r(s,a,s^{'})$.可以通过贝尔曼方法来迭代计算其值函数。

动态规划来计算，常用的方法有：

- 策略迭代：每次迭代分成两步

  - 策略评估：计算当前策略下，每个状态的值函数，即算法14.1中的3-6步。策略评估可以通过贝尔曼方程（公式14.18）进行迭代计算$V^{\pi}(s)$.

    如果状态数有限时，也可以通过直接求解bellman方程来得到$V^{\pi }(s)$.

  - 策略改进:根据值函数来更新策略，即算法14.1中的7-8步。

  策略迭代算法14.1所示。

  ![1565073819732](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565073819732.png)

  

- 值迭代：**将策略评估和策略改进两个过程合并**，来直接计算出最优策略。主要针对策略迭代算法中策略评估采用一个内部迭代来进行计算，其计算量比较大。事实上，不需要每次计算出每次策略对应的精确的值函数，也就是说内部迭代不需要执行到完全收敛。

  假设最优策略$\pi^{*}$对应的值函数称为最优值函数，那么最优状态值函数$V^{*}(s)$和最优状态-动作值函数$Q^{*}(s,a)$的关系为
  $$
  V^{*}(s)=\mathop{max}_a Q^*(s,a).
  $$
  ![1565075962820](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565075962820.png)

![1565076250784](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565076250784.png)

### **24、策略迭代 VS 值迭代**

- 时间复杂度：值迭代每次时间复杂度要比策略迭代少很多，但是迭代次数要比策略迭代更多。
- 策略：值迭代采用贝尔曼最优方程来更新值函数，收敛时的值函数就是最优的值函数，其对应的策略就是最优的策略。策略迭代是根据贝尔曼方程来更新值函数，并根据当前的值函数来改进策略。

![1565077231096](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565077231096.png)

基于模型的强化学习算法是一种动态规划方法，实际使用汇总有两点限制。

- **要求模型已知。**即要给出马尔科夫决策过程的状态转移概率$p(s^{'}|s,a )$和奖励函数$r(s,a,s^{'})$,这个很难满足。
- **效率问题。** 当状态数量较大的时候，算法的效率比较低。

---------

### **25、 蒙特卡罗方法**（**模型无关**）

很多应用场景中，马尔科夫决策过程的状态转移概率和奖励函数都是未知的。这种情况下需要智能体与环境交互，并收集一些样本。然后再根据这些样本来求解马尔可夫决策过程的最优策略。

**基于采样的学习算法也称为模型无关的强化学习**

![1565078403381](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565078403381.png)

![1565079376816](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565079376816.png)

![1565079399336](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565079399336.png)





### **26、 时序差分学习方法**

蒙特卡罗采样方法：一般需要拿到完整的轨迹，才能对策略进行评估并更新模型，因此效率也比较低。

时序差分学习：$\color{red}结合了动态规划和蒙特卡罗方法$，比仅仅使用蒙特卡罗效率高很多。

**时序差分学习是模拟一段轨迹，每行动一步（或者几步），就利用贝尔曼方程来评估行动前状态的价值。**

![1565080905544](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565080905544.png)

![1565082210546](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565082210546.png)

![1565082329897](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565082329897.png)

![1565082371821](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565082371821.png)



![1565083467118](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565083467118.png)

$\color{red}Q学习算法$ 

![1565147379275](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565147379275.png)

**这两个算法的流程基本一致，唯一不同在于Q函数的更新：**

Q-learning在计算下一状态的预期收益时使用了max操作，直接选择最优动作，而当前policy并不一定能选择到最优动作，因此这里生成样本的policy和学习时的policy不同，为off-policy算法；

而SARAS则是基于当前的policy直接执行一次动作选择，然后用这个样本更新当前的policy，因此生成样本的policy和学习时的policy相同，算法为on-policy算法。

而最近深度强化学里中使用的experience-replay机制将生成的样本与训练的样本独立开来，使用某一policy生成的样本拿来训练的时候，很可能当前policy已经和之前有所差别，因此使用experience-replay机制的DRL算法基本上是off-policy算法



### 27、深度Q网络

> DQN结合代码看的话，DRL-FlappyBird代码层次强，逻辑清晰。具体地址如下：
>
> https://github.com/floodsung/DRL-FlappyBird

为了在连续的状态和动作空间中计算值函数$Q^{\pi}(s,a)$,采用一个函数$Q_{\phi}(s,a)$来表示近似计算，称为**值函数近似**。
$$
Q_{\phi}(s,a) \approx Q^{\pi}(s,a)
$$
其中**s，a**分别是状态s和动作a的向量表示；函数$Q_\phi(s,a)$通常是一个参数为$\phi$的函数，比如神经网络，输出为一个实数，称为Q网络。

如果动作为有限离散的m个动作a1,...,a_m，我们可以让Q网络输出一个m维向量，其中每一维都用$Q_\phi (s,a_i)$来表示，对应值函数$Q(s,a_i)$的近似值。

![1565158937785](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565158937785.png)

![1565158984807](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565158984807.png)

以Q学习为例，采用随机梯度下降，目标函数为
$$
L(s,a,s^{’})=(r+\gamma \mathop\max_{a^{'}} Q_\phi(s^{'}，a^{'})-Q_\phi(s,a))^2,
$$
其中$s^{'},a^{'}$是下一时刻的状态$s^{'}和动作a^{'}$ 的向量表示。

**目标函数问题：**

- 目标不稳定，参数学习的目标依赖于参数本身；
- 样本之间有很强的相关性。

解决方法：提出了DQN

- **目标网络冻结：**即在一个时间段内固定目标中的参数，稳定学习目标；
- **经验回放：**由智能体最近的经历组成的数据集，去除数据之间的相关性。

![1565159714656](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565159714656.png)



>基于值函数的学习方法中，策略一般为**确定性的策略**。策略优化通常依赖于值函数，比如贪心策略$\pi(s)=\mathop \ arg \max_aQ(s,a)$.最优策略一般需要遍历当前状态s下的所有动作，并找出最优的Q(s,a)。
>
>如果动作空间离散但是很大，那么遍历求最大需要很高的时间复杂度；
>
>如果动作空间连续并且Q(s,a)非凸时，也很难求解出最佳的策略。

### 28、基于策略函数的学习方法

强化学习目标：学习一个策略$\pi_\theta(a|s)$来最大化期望回报。

策略搜索：一种直接方法，策略空间中直接搜索得到最佳策略。本质是优化问题，分为基于梯度的优化和无梯度优化。不需要值函数，直接优化策略。

参数化的策略能够处理连续状态和动作，可以直接学出随机性策略。

策略梯度：基于梯度的强化学习方法，假设$\pi_\theta(a|s) $是一个关于$\theta $的连续可微函数，可以用**梯度上升**的方法来优化参数$\theta$ 使得目标函数$J(\theta)$最大。

![1565161164248](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565161164248.png)

目标函数$J(\theta )$关于策略参数$\theta $的导数为

![1565161262128](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565161262128.png)

![1565161496526](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565161496526.png)

![1565161694920](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565161694920.png)

![1566528039347](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566528039347.png)

![1566532086640](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566532086640.png)

REINFORCE算法缺点：

- 不同路径之间的方差很大，导致训练不稳定，这是在高纬空间中使用蒙特卡罗方法的通病。解决方法，加入一个控制变量，减少方差。

### 29、Actor-Critic算法

针对REINFORCE算法方差大，效率低的缺点，借鉴时序差分学习思想，使用动态规划方法来提高采样效率，即从状态开始s的总回报可以通过当前动作的即时奖励$r(s,a,s^‘)$和下一个状态$s^{'} $的值函数来近似估计。 

演员-评论员算法（AC）：结合**策略梯度**和**时序差分学习**的强化学习算法。

演员：指的是策略函数$\pi_\theta（s,a）$，即学习一个策略来得到尽量高的回报。

评论员：指的是值函数$V_\theta(s)$，对当前策略的值函数进行估计，即评估actor的好坏。

借助于值函数，AC算法可以进行**单步更新参数**，不需要等到回合结束才进行更新。



在AC算法总，策略函数和值函数都是待学习的函数，需要在训练过程中同时学习。

![1565228256769](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565228256769.png)

![1565228382050](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565228382050.png)

actor-critic算法的训练过程如下：

![1565228592364](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565228592364.png)

David Silver版本的AC算法如下：

![1566541310421](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566541310421.png)

虽然在带基准线的REINFORCE算法也同时学习策略函数和值函数，但是
它并不是一种Actor-Critic 算法。因为其中值函数只是用作基线函数以减少方
差，并不用来估计回报（即评论员的角色）。

>结合代码看的话，dennybritz应该写的比较好，思路清晰，层次分明，地址如下：
>
>https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb

https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f

![1566542595477](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566542595477.png)

![1566543496374](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566543496374.png)

sutton（2018）版本的AC算法如下，主要是策略的更新，少了一项因子，Q(W,A)。实验中表明，如果加上了这一项程式因子，训练比较快速，但是效果并不是很好，rewards值很小。

![1566785801653](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566785801653.png)

代码实际测试中，策略梯度的更新，如果乘以了当前的动作值函数，网络更新非常快，但是效果并不是很好，reward很小

代码如下：

```

#%% Change working directory from the workspace root to the ipynb file location. Turn this addition off with the DataScience.changeDirOnImportExport setting
import os
try:
	os.chdir(os.path.join(os.getcwd(), 'PolicyGradient'))
	print(os.getcwd())
except:
	pass

#%%
# get_ipython().run_line_magic('matplotlib', 'inline')

import gym
import itertools
import matplotlib
import numpy as np
import sys
import tensorflow as tf
import collections

if "../" not in sys.path:
  sys.path.append("../") 
from lib.envs.cliff_walking import CliffWalkingEnv
from lib import plotting

matplotlib.style.use('ggplot')


#%%
env = CliffWalkingEnv()


#%%
class PolicyEstimator():
    """
    Policy Function approximator. 
    """
    
    def __init__(self, learning_rate=0.01, scope="policy_estimator"):
        with tf.variable_scope(scope):
            self.state = tf.placeholder(tf.int32, [], "state")
            self.action = tf.placeholder(dtype=tf.int32, name="action")
            self.target = tf.placeholder(dtype=tf.float32, name="target")

            # This is just table lookup estimator
            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))  # 状态空间
            
            self.output_layer = tf.contrib.layers.fully_connected(
                inputs=tf.expand_dims(state_one_hot, 0),
                num_outputs=env.action_space.n,
                activation_fn=None,
                weights_initializer=tf.zeros_initializer)

            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))
            self.picked_action_prob = tf.gather(self.action_probs, self.action)

            # Loss and train op
            self.loss = -tf.log(self.picked_action_prob) * self.target

            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
            self.train_op = self.optimizer.minimize(
                self.loss, global_step=tf.contrib.framework.get_global_step())
    
    def predict(self, state, sess=None):
        sess = sess or tf.get_default_session()
        return sess.run(self.action_probs, { self.state: state })

    def update(self, state, target, action, sess=None):
        sess = sess or tf.get_default_session()
        feed_dict = { self.state: state, self.target: target, self.action: action  }
        _, loss = sess.run([self.train_op, self.loss], feed_dict)
        return loss


#%%
class ValueEstimator():
    """
    Value Function approximator. 
    """
    
    def __init__(self, learning_rate=0.1, scope="value_estimator"):
        with tf.variable_scope(scope):
            self.state = tf.placeholder(tf.int32, [], "state")
            self.target = tf.placeholder(dtype=tf.float32, name="target")

            # This is just table lookup estimator
            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))
            self.output_layer = tf.contrib.layers.fully_connected(
                inputs=tf.expand_dims(state_one_hot, 0),
                num_outputs=1,
                activation_fn=None,
                weights_initializer=tf.zeros_initializer)

            self.value_estimate = tf.squeeze(self.output_layer)
            self.loss = tf.squared_difference(self.value_estimate, self.target)

            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
            self.train_op = self.optimizer.minimize(
                self.loss, global_step=tf.contrib.framework.get_global_step())        
    
    def predict(self, state, sess=None):
        sess = sess or tf.get_default_session()
        return sess.run(self.value_estimate, { self.state: state })

    def update(self, state, target, sess=None):
        sess = sess or tf.get_default_session()
        feed_dict = { self.state: state, self.target: target }
        _, loss = sess.run([self.train_op, self.loss], feed_dict)
        return loss


#%%
def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):
    """
    Actor Critic Algorithm. Optimizes the policy 
    function approximator using policy gradient.
    
    Args:
        env: OpenAI environment.
        estimator_policy: Policy Function to be optimized 
        estimator_value: Value function approximator, used as a critic
        num_episodes: Number of episodes to run for
        discount_factor: Time-discount factor
    
    Returns:
        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.
    """

    # Keeps track of useful statistics
    stats = plotting.EpisodeStats(
        episode_lengths=np.zeros(num_episodes),
        episode_rewards=np.zeros(num_episodes))    
    
    Transition = collections.namedtuple("Transition", ["state", "action", "reward", "next_state", "done"])
    
    for i_episode in range(num_episodes):
        # Reset the environment and pick the fisrst action
        state = env.reset()
        
        episode = []
        
        # One step in the environment
        for t in itertools.count():
            
            # Take a step
            action_probs = estimator_policy.predict(state)
            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)
            next_state, reward, done, _ = env.step(action)
            
            # Keep track of the transition
            episode.append(Transition(
              state=state, action=action, reward=reward, next_state=next_state, done=done))
            
            # Update statistics
            stats.episode_rewards[i_episode] += reward
            stats.episode_lengths[i_episode] = t
            
            # Calculate TD Target
            value_next = estimator_value.predict(next_state)
            td_target = reward + discount_factor * value_next
            td_error = td_target - estimator_value.predict(state)
            
            # Update the value estimator
            estimator_value.update(state, td_target)
            
            # Update the policy estimator
            # using the td error as our advantage estimate
            estimator_policy.update(state, td_error, action)
            
            # Print out which step we're on, useful for debugging.
            print("\rStep {} @ Episode {}/{} ({})".format(
                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end="")

            if done:
                break
                
            state = next_state
    
    return stats


#%%
tf.reset_default_graph()

global_step = tf.Variable(0, name="global_step", trainable=False)
policy_estimator = PolicyEstimator()
value_estimator = ValueEstimator()

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    # Note, due to randomness in the policy the number of episodes you need to learn a good
    # policy may vary. ~300 seemed to work well for me.
    stats = actor_critic(env, policy_estimator, value_estimator, 2000)


#%%
plotting.plot_episode_stats(stats, smoothing_window=10)



```

![1566787407644](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566787407644.png)

![1566786701859](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566786701859.png)

![1566786716174](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566786716174.png)

### 30、总结

![1565228786269](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565228786269.png)

不同强化学习算法步骤可以分为三步

1）执行策略

2）估计回报

3）更新策略

![1565228920771](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565228920771.png)

![1565229717518](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1565229717518.png)

**逆向强化学习：** 

强化学习的基础是智能体可以和环境进行交互，得到奖励。但在某些情况下，智能体无法从环境得到奖励，只有一组轨迹示例（demonstration）。比如在自动驾驶中，我们可以得到司机的一组轨迹数据，但并不知道司机在每个时刻得到的即时奖励。虽然我们可以用监督学习来解决，称为行为克隆。但行为克隆只是学习司机的行为，并没有深究司机行为的动机。

逆向强化学习（IRL）就是指一个**不带奖励的马尔科夫决策过程**，通常给定的一组专家（或教师）的行为轨迹示例来逆向**估计出奖励函数$r(s,a,s^{'})$ **来解释专家的行为，然后再进行强化学习。

**分层强化学习**：

分层强化学习是指将一个复杂的强化学习问题分解成多个小的、简单的子问题，每个子问题都可以单独用**马尔可夫决策过程**来建模。这样，我们可以将智能体的策略分为高层次策略和低层次策略，高层次策略根据当前状态决定如何执行低层次策略。这样，智能体就可以解决一些非常复杂的任务。

---------------------------

-----------------

------------------

-----------------

--------------------

## 四、前馈神经网络

### 1、神经元及激活函数

**人工神经网络**（Artificial Neural Network，ANN）是指一系列受生物学和神经学启发的数学模型。

**人工神经元**（Artificial Neuron），简称神经元（Neuron），是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接受一组输入信号并产出输出。



**激活函数**在神经元中非常重要的。为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质：

1. 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。
2. 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。
3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。



Logistic 函数和Tanh 函数都是Sigmoid 型函数，具有饱和性，但是计算开销较大。



**修正线性单元**

![1566308865191](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566308865191.png)



**优点**：采用ReLU 的神经元只需要进行加、乘和比较的操作，计算上更加高效。ReLU函数被认为有生物上的解释性，比如单侧抑制、宽兴奋边界（即兴奋程度也可以非常高）。在生物神经网络中，同时处于兴奋状态的神经元非常稀疏。人脑中在同一时刻大概只有1 ∼ 4%的神经元处于活跃状态。Sigmoid 型激活函数会导致一个非稀疏的神经网络，而ReLU却具有很好的稀疏性，大约50% 的神经元会处于激活状态。在优化方面，相比于Sigmoid 型函数的两端饱和，ReLU函数为左饱和函数，且在x > 0 时导数为1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。

**缺点**：ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。此外，ReLU神经元在训练时比较容易“死亡”。在训ReLU 神经元指采用ReLU练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU神经元在 作为激活函数的神经元。所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。这种现象称为死亡ReLU问题（DyingProblem），并且也有可能会发生在其它隐藏层。



![1566310239919](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566310239919.png)



![1566310352394](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566310352394.png)

![1566310678840](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566310678840.png)

![1566312088871](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566312088871.png)



### 2、网络结构

**前馈网络：**前馈网络中各个神经元按接受信息的先后分为不同的组。每一组可以看作一个神经层。每一层中的神经元接受前一层神经元的输出，并输出到下一层神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播，可以用一个有向无环路图表示。前馈网络包括全连接前馈网络[本章中的第4.3节] 和卷积神经网络[第5章] 等。
前馈网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单，易于实现。

**反馈网络：**反馈网络不仅接收前一层神经元的输出，同时接收自身的反馈。和前馈网络相比，反馈网络中的神经元具有记忆功能，在不同的时刻具有不同的状态。反馈神经网络中的信息传播可以是单向或双向传递，因此可用
一个有向循环图或无向图来表示。反馈网络包括循环神经网络[第6章]，Hopfield网络[第6章]、玻尔兹曼机[第12章] 等。

**图神经网络：**前馈网络和反馈网络的输入都可以表示为向量或向量序列。但实际应用中很多数据是图结构的数据，比如知识图谱、社交网络、分子（molecular ）网络等。前馈网络和反馈网络很难处理图结构的数据。图网络是定义在图结构数据上的神经网络[第6.8.2节]。图中每个节点都一个或一组神经元构成。节点之间的连接可以是有向的，也可以是无向的。每个节点可以收到来自相邻节点或自身的信息。图网络是前馈网络和记忆网络的泛化，包含很多不同的实现方式，比如图卷积网络（Graph Convolutional Network，GCN）[Kipf and Welling, 2016]、消
息传递网络（Message Passing Neural Network，MPNN）[Gilmer et al., 2017]等。

![1566349104169](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566349104169.png)

### 3、误差反向传播算法

![1566435401644](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566435401644.png)



![1566435458309](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566435458309.png)



![1566436229419](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566436229419.png)



![1566436248061](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566436248061.png)



![1566436263235](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566436263235.png)



![1566436282994](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566436282994.png)





![1566439442489](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566439442489.png)

## 六、循环神经网络

>  经验是智慧之父，记忆是智慧之母

### 33、循环神经网络

循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其它神经元的信息，也可以接受自身的信息，形成具有环路的网络结构.

![1566350636375](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566350636375.png)

循环神经网络可以应用到很多不同类型的机器学习任务。根据这些任务的特点可以分为以下几种模式：

- 序列到类别模式(分类问题)
- 同步的序列到序列模式
- 异步的序列到序列模式。

![1566351634653](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566351634653.png)

![1566351660220](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566351660220.png)

![1566351804561](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566351804561.png)

### 34、参数学习

两种参数学习方法

- 随时间反向传播算法：看成展开的多层前馈网络，反向传播算法来计算梯度
- 实时学习循环算法：前向传播的方式计算梯度；

![1566353439819](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566353439819.png)

![1566353453562](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566353453562.png)

![1566353469174](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566353469174.png)

![1566353487349](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566353487349.png)

（2）实时在线循环学习

![1566354491808](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566354491808.png)

![1566354514539](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566354514539.png)

### 35、长短期记忆网络（LSTM）

![1566357498786](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357498786.png)

为了解决上节中提到的记忆容量问题，一种非常好的解决方案是引入门控Hochreiter and Schmidhuber [1997] 来控制信息的累积速度，包括有选择地加入新的信息，并有选择地遗忘之前累积的信息。这一类网络可以称为基于门控的循环神经网络（Gated RNN）。本节中，主要介绍两种基于门控的循环神经网络：长短期记忆（LSTM）网络和门控循环单元（GRU）网络。

LSTM可以解决简单循环神经网络中的梯度爆炸和梯度消失问题，主要有两点改进：

- 新的内部状态
- 门机制

![1566357164144](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357164144.png)

![1566357195636](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357195636.png)

![1566357234128](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357234128.png)



![1566357253539](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357253539.png)



### 36、深层循环神经网络

6.7.1 堆叠循环神经网络

![1566357739849](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357739849.png)

![1566357944919](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566357944919.png)

### 37、扩展到图结构

![1566358146878](F:\软件所工作\iscas-DL\机器学习与深度学习.assets\1566358146878.png)




## 附录

### 1、贝叶斯定理

贝叶斯定理是关于随机事件A和B的[条件概率](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87)的一则定理。 
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$
其中P(A|B)是指在事件B发生的情况下事件A发生的概率。 

在贝叶斯定理中，每个名词都有约定俗成的名称： 

- P(*A*|*B*)是已知B发生后A的[条件概率](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87)，也由于得自B的取值而被称作A的[后验概率](https://zh.wikipedia.org/wiki/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87)。
- P(*A*)是A的[先验概率](https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87)（或[边缘概率](https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87)）。之所以称为"先验"是因为它不考虑任何B方面的因素。
- P(*B*|*A*)是已知A发生后B的条件概率，也由于得自A的取值而被称作B的[后验概率](https://zh.wikipedia.org/wiki/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87)。
- P(*B*)是B的[先验概率](https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87)。

 按这些术语，贝叶斯定理可表述为： 

​	 **后验概率 = (似然性*先验概率)/标准化常量**


 也就是说，后验概率与先验概率和相似度的乘积成正比。 

另外，比例P(*B*|*A*)/P(*B*)也有时被称作标准似然度（standardised likelihood），贝叶斯定理可表述为： 

 	 **后验概率 = 标准似然度*先验概率***































