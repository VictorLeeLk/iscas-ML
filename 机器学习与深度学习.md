**1、成本函数与损失函数区别**

- **损失函数(Loss function)**是定义在**单个训练样本上**的，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的，用L表示。当前也有应用到整个数据集上的损失。
- **代价函数(Cost function)**是定义在**整个训练集上面**的，也就是所有样本的误差的总和的平均，也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。只能应用到整个数据集，可以有更复杂的组合和计算，可以加上正则化项。

2、机器学习的目标是找到一个模型来近似真实映射函数g(x)或真实条件概率分布pr(y|x)。

**3、机器学习的三个基本要素**

- $\color{red}模型:$不知道真实的映射函数g(x)或条件概率分布pr(y|s)的具体形式，根据经验来确定一个假设函数集合F,称为**假设空间**，然后通过观测其在训练集D上的特性，从中选择一个理想的假设$$f^*\in F$$。常见的假设空间可以分为线性模型和非线性模型（如神经网络模型，**假设输入到输出的表达关系$f(x,\theta)$**）；

  假设空间F通常为一个参数化的函数族 $$F=\{f(x,\theta)|\theta\in R^m)\}$$ 。类似于神经网络训练过程中不同的参数集合。

- $\color{red}学习准则（损失函数）：$期望风险最小化，有0-1损失函数、交叉熵损失、平方差损失等。在训练数据集上，通过经验风险最小化来近似期望风险最小化，由于过拟合问题，通过增加正则化，设计结构风险最小化。**相当于设计目标函数（最优化问题）。** 学习准则有经验风险最小化、结构风险最小化、最大似然估计、最大后验估计等。

- $\color{red}优化算法：$在模型假设空间上找到最优化的模型，也就是找到使得结构风险最小**的参数集合**。如SGD算法。**（求解最优化问题）**  

**4、过拟合**

- 过拟合：根据大数定理，当训练集大小|D|趋向无穷大，经验风险就趋向于期望风险。然而通常，无法取得无限的训练    样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好地反映全部数据的真实分布。经验风险最小化原则很容易导致模型在训练数据集上错误率较低，而在测试集上错误率较高。

- **原因：**训练数据少、噪声、模型能力强等造成

**5、优化算法**

机器学习的训练过程其实就是最优化问题的求解过程

优化可以分为**参数优化**和**超参数优化**

- 参数优化：**模型$$f(x,\theta)$$** 中的$$\theta$$
- 超参数优化: 定义**模型结构或优化策略**的参数。常见的超参数包括如下
  - 聚类算法中的类别的个数
  - 梯度下降法的步长
  - 正则项的系数
  - 神经网络的层数
  - ...........

**6、梯度下降法** 

凸优化：选择合适模型和损失函数构造凸优化目标，从而利用一些高效、成熟的优化方法，如共轭梯度、拟牛顿法等

非凸优化：神经网络，只能找到局部最优解。

最简单、最常用的优化算法就是**梯度下降法**，通过迭代来计算训练集D上风险函数的最小值。

$$\theta_{t+1}=\theta_{t}-\alpha\frac{\alpha R_D(\theta)}{\alpha\theta}$$

$$=\theta_t-\alpha.\frac{1}{N}\sum^N_{n=1}\frac{\alpha L(y^{(n)},f(x^{n},\theta))}{\alpha \theta}$$



**7、提前终止**

**目的：** 防止过拟合

**过程：** 除了训练集和测试集之外，有时会采用一个验证集来进行模型选择，测试模型在验证集上是否最优。在每次迭代时，把最新得到的模型$$f(x,\theta)$$在验证集上进行测试，并计算错误率。如果在验证集上错误率不再下降，就停止迭代。这种策略叫做提前终止。

![提前终止示意图](F:\软件所工作\iscas-DL\机器学习与深度学习.assets/提前终止示意图.png)

**8、随机梯度下降法**

- **批量梯度下降法** （Batch Gradient Descent,BGD）:目标函数是整个训练集上风险函数。每次迭代需要计算每个样本上损失函数的梯度并求和。

  **缺陷：**当训练集样本数量N较大时，空间复杂度比较高，每次迭代计算开销很大。

- **随机梯度下降法（Stochastic Gradient Descent,SGD）:** 每次迭代只采集一个样本，计算这个样本损失函数的梯度并更新参数。经过足够次数的迭代时，随机梯度下降可以收敛到局部最优解。[Nemirovski et al., 2009]。一个缺点是无法充分利用计算机的并行计算能力。

![随机梯度下降算法](F:\软件所工作\iscas-DL\机器学习与深度学习.assets/随机梯度下降算法.png)

**两者区别**： 优化目标是所有样本的平均损失还是单个样本的损失。随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声。当目标函数非凸时，反而可以使其逃离局部最优点。

**小批量梯度下降法（Mini-Batch GD）:** 随机梯度和批量梯度的折中，每一次迭代选取一小部分训练样本来计算梯度并跟新参数。计算高效，收敛快，开销小，成为了大规模机器学习中的主要优化算法。

**8、线性回归**

假设空间是一组参数化的线性函数$$f(x;w,b)=w^Tx+b$$,其中w,b可以学习。

$$f(x;w,b)\in R$$称之为线性模型。

简单起见，可写成$$f(x,w)=\hat{w}^T.\hat{x}$$

其中，$$\hat{w}$$和$$\hat{x}$$ 分别称之为**增广权重向量**和**增广特征向量**

$$\hat{x}=x\bigoplus1\buildrel \Delta \over =\begin{bmatrix} \\\\x \\ \\\\ \\1\\ \end{bmatrix}=\begin{bmatrix} x_1\\.\\.\\.\\\\ x_k\\1\\ \end{bmatrix}$$



$$\hat{w}=w\bigoplus b\buildrel \Delta \over =\begin{bmatrix} \\\\w \\ \\\\ \\b\\ \end{bmatrix}=\begin{bmatrix} w_1\\.\\.\\.\\\\ w_k\\1\\ \end{bmatrix}$$

一般来说，后面直接用w和b代替增广权重向量和增广特征向量。

线性回归模型可以写成$$f(x;w)=w^Tx$$

- 参数学习

  给定一组包含N个训练样本的训练集$$D=\{({x^{(n)},y^{(n)}})\},1\le n \le N$$

  希望能够学习一个最优的线性回归的模型参数w。

  四种不同参数估计方法

  - 经验风险最小化
  - 结构风险最小化
  - 最大似然估计
  - 最大后验估计

- 经验风险最小化

​                   $$R(w)=\sum\limits_{n=1}^{N} {L(y^{(n)},f(x^{(n)},w))} \\$$

​                             $$=\frac{1}{2} \sum\limits_{n=1}^{N}{(y^{(n)}-w^Tx^{(n)})^2}$$	

​			     $$=\frac{1}{2}||y-X^Tw||^2$$		

其中$y\in R^N$是每个样本的真实标签$y^{(1)},\cdots,y^{(N)}$组成的列向量，$X\in R^{(d+1)\times N}$是所有输入$x^{(1)},\cdots,x^{(N)}$

组成的矩阵。**$$\color{red}矩阵的每一列代表一个样本的特征向量。$$**
$$
X=
\begin{bmatrix} 
	x_1^{(1)}&x_1^{(2)}& \cdots & x_1^{(N)}\\ 
	\vdots &\vdots & \ddots & \vdots\\ 
	x_d^{(1)}& x_d^{(2)}&\cdots& x_d^{(N)}\\
	1  & 1& \cdots &1  \ 
\end{bmatrix}
$$
风险函数R(w)是关于w的凸函数，其对w的偏导数为
$$
\begin{align}
\frac{\partial R(w)}{\partial w}&=\frac{1}{2}\frac{\partial||y-X^Tw||^2}{\part w} \\
 &  = -X(y-X^Tw)
 \end{align}
$$
令$\frac{\part}{\part w}R(w) =0$,得到最优的参数$w^*$为
$$
w^*=(XX^T)^{-1}Xy
$$
其中$(XX^T)^{-1}X$也称之为X的**伪逆矩阵**。



**9、线性代数基础** 

**向量：**一组实数组成的有序数组，同时具有大小和方向。

**向量空间（线性空间）：**指由向量组成的集合，满足以下两个条件：

- 向量加法：向量空间V中的两个向量a和b,它们的和a+b也属于空间V;
- 标量乘法：向量空间V中的任一向量a和任一标量c,它们的乘积$c\cdot a$也属于空间V

**范数：** 表示向量“长度”的函数，为向量空间内的所有向量赋予非零的正长度或大小。对于一个n维向量v,一个常见的范数函数为$l_p​$范数，
$$
l_p\equiv||v||_p=(\sum\limits_{i=1}^{n}{|v_i|^p})^{1/p}
$$
其中$p\ge 0$为一个标量的参数。常用的p取值有1,2，$\infty $等。

- $l_1$范数：各个元素的绝对值之和
- $l_2$范数：根号下各个元素的平方和
- $l_\infty$范数：向量各个元素的最大绝对值



























